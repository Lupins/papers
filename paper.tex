\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Paper Title*\\
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
Multi-stream, optical flow, saliency, video, deep learning
\end{IEEEkeywords}

\section{Introduction}

Ambient assisted living technologies have been rising in popularity as the world population grows older and more robust solutions became available. 

\section{Related Work}

% threshold or ROI
Sase et al.~\cite{sase2018human} detected regions of interest using background subtraction. If the region of interest was bellow a threshold a fall event was accused, the threshold was on third of the subject height in the frame.

Bhandari et al.~\cite{bhandari2017novel} used Shi-Tomasi to determine regions of interest and Lucas-Kanade to track them. If the speed and motion of the region of interest surpassed a threshold, a fall event was detected.

% SVM
Zerrouki and Houacine~\cite{zerrouki2018combined} subtracted the background, applied a curvelet transform on this information and used a support vector machine (SVM) to classify the human pose. The classified pose was then passed to a hidden markov model, that recognized the activity.

Qian et al.~\cite{qian2017recognizing} also employed a hidden markov model to detect falls.

Yu et al.~\cite{yu2009fall} utilized particle filtering with a background subtraction to track the subject head, and detect falls.

Yu et al.~\cite{yu2010robust} applyed a head tracking based on a gaussian model with a parzen window method to calculate density.

Rougier et al.~\cite{rougier2011robust} detected falls with gaussian and human shape deformation.

Regarding the subject's privacy, both Edgcomb et al.~\cite{edgcomb2012automated} and Lin et al.~\cite{lin2013fall} investigated privacy-prone features, such as bluring, silhouetting, and covering the body with shapes.

Searching for a higher sensitivity, Harrou et al.~\cite{harrou2017vision} segmented the subject in the video and served it to a feature extractor. The extractor output would be compared with a employed multivariate exponentially-weighted moving average (MEWMA), and if it was considered alarming a SVM would then verify if a fall occured.

In a mixed approach, Kwolek and Kepski~\cite{kwolek2015improving} used an accelerometer, that passed a given threshold would activate a K-nearest neighbors vision classifier to confirm the event.

% CNN
N\'u\~nez-Marcos et al.~\cite{nunez2017vision} extracted dense optical flow (OF) from images and fed them to a VGG-16 convolutional neural network (CNN). The extracted optical flow was fed in stacks to create a temporal relation between frames.

Anishchenko et al.~\cite{anishchenko2018machine} employd a modified AlexNet to detect falls.

\section{Background}

In this section, we describe a few relevant concepts applied in the proposed method.

\subsection{Optical Flow}
\label{sec:opticalflow}

Optical flow is a hand-crafted feature that conveys movement information between the frames of a video, either by object movement or camera shift, Figure~\ref{fig:opticalflow}. Its ability to describe movement enables this feature to support the deep network towards detecting falls.

The extraction process considers a video frame as $I$ and $I(x, y, t)$ a pixel in this frame. Another frame collected $dt$ time later, and the distance that the same pixel moved $(dx, dy)$, described in Equation~\ref{eq:of-dist}.

Optical flow Equation~\ref{eq:of} is formed after applying a Taylor series approximation of right-hand side and dividing by $dt$, $f_t$ is the gradient given time, $f_x$, $f_y$, $u$, and $v$ are given in Equation~\ref{eq:of-grad}. Values of $u$ and $v$ can be achieved by different methods, such as the classical Lucas-Kanade~\cite{jain2018abnormal} or the one used in this work: Gunnar-Farneb{\"a}ck~\cite{lowhur2015dense}.

\begin{equation}
\label{eq:of-dist}
I(x, y, t)=I(x+dx, y+dy, t+dt)
\end{equation}
\begin{equation}
\label{eq:of}
f_xu + f_yv + f_t=0
\end{equation}
\begin{equation}
\label{eq:of-grad}
f_x = \frac{\partial f}{\partial x}; \quad f_y = \frac{\partial f}{\partial y}u = \frac{\partial x}{\partial t}; \quad v = \frac{\partial y}{\partial t}
\end{equation}

\subsection{Saliency Map}
\label{sec:saliency}

Saliency was envisioned as a visualizing technique to understand the learning process behind a deep network. It can be extracted in many ways, one of those is the practice of feeding a image $I$ to the network and its class label $c$, freeze the weights on the network and compute the gradients of $c$ with respect to the pixels of $I$. Consider $S_c(I)$ to be the class score function for an image $I$ and, for this simple explanation, that it is a simple linear function, $S_c(I) = w_c^{T} + b_c$. With $w$ being the weights of the network and $b$ the bias, it is possible to see that the magnitude of elements of $w$ affects the importance of the corresponding pixels in $I$ for class $c$.

The saliency map was chosen as a stream for its spatial relation to the frame, as the raw RGB frames are prone to interference, for instance from brightness variation, we believe the saliency is capable to overcome these variables. This work's saliency extractor is implemented after the paper of Sundararajan et al.~\cite{sundararajan2017axiomatic}.

\section{Method}

An ensemble of VGG-16s were employed and evaluated to classify fall and not fall in video sequences, as seen in Figure~\ref{fig:overview} and further detailed bellow.

% In this work we intended to employ and evaluate a CNN architecture capable of detecting the presence or not of falls in videos, thus approaching the problem as a binary classification. To this end, we select the CNN architecture VGG16, since it is well known, and yields satisfactory results. Furthermore, based on the results of Wang et al.~\cite{wang2015towards}, in which a multi-streamed CNN outperformed single-stream ones, a ensembled was also exerted to learn the influence of three selected streams, each stream fed with a hand-crafted feature, them being: (i) optical flow, (ii) rgb, and (iii) saliency map.

\subsection{Transfer Learning}

The selected datasets, further detailed in Section~\ref{sec:experiments}, possesses a relatively small amount of human fall samples in their videos, which would limit the CNNs learning capabilities to detect a fall event. Hence, the importance of transfer learning.

The first 14 layers of the VGG-16 architecture were trained on the ImageNet~\cite{imagenet_cvpr09} dataset, and later on, these same layers were fine-tuned over the UCF101~\cite{soomro2012ucf101} dataset. The reasoning being that the network would initially learn low level features from the ImageNet classes, and focus these features on human activities with the UCF101 videos, Figure~\ref{fig:pre14}.

The 14 initial layer's weights were then frozen, and the last two fully connected layers were separately fine-tuned, with dropout regularization, over the extracted hand-crafted features of the URFD~\cite{kepski2014human} and FDD~\cite{charfi2013optimised} datasets. The last fine-tuning was performed three times, one for each of the extracted hand-crafted features, Figure~\ref{fig:pos14}.

\subsection{Classification}
\label{sec:classification}

Wang et al.~\cite{wang2015towards} reported that networks employing multi-stream classifiers outperformed their single-streamed peers. Inspired on this finding, a SVM classifier fed with the outputs of each stream, between 0 and 1, generates a ($x$, $y$, $z$) vector. This vector is then fed to a second SVM that classifies between fall and not fall, Figure~\ref{fig:pos14}.

\subsection{Streams Input}

Two selected features were fed to the streams explained in Subsection~\ref{sec:classification}:(i) Optical Flow and (ii) Saliency Map. Furthermore, the RGB frame itself was fed to the third stream, Figure~\ref{fig:features}, given the neural networks capability to learn relevant features to describe their input.

\paragraph{Optical Flow} represents the temporal relation between frames, and to fully gather this property it was used as a stack of optical flows on top of a sliding window of frames. The sliding window has a size of $L$ frames and gathers 2$L$ components ($L$ horizontal ($d_t^x$) + $L$ vertical ($d_t^y$) optical flow component vector fields) creating a stack $O$ = \{$d_t^x$, $d_t^y$, $d_{t+1}^x$, $d_{t+1}^y$, ..., $d_{t+L}^x$, $d_{t+L}^y$ \}. The total number of stacks is given by $N$-$L$+$1$, where $N$ is the number of frames in a video, Figure~\ref{fig:stacks, fig:features}. 
\paragraph{Saliency Map} is composed of pixels varying from 0 to 1, and was fed as a gray-scale image to the network, Figure~\ref{fig:features}.

\section{Experiments}
\label{sec:experiments}

Our experiments were performed on two publicly available datasets, \textit{URFD}~\cite{kepski2014human} and \textit{FDD}~\cite{charfi2013optimised}.

\paragraph{URFD dataset} contains 70 videos: (i) 30 videos of falls; and (ii) 40 videos of daily living activities, recorded from a top-down and side view perspective by two Microsoft Kinect cameras. Each frame is labeled in three categories, whether the person is lying on the ground, not lying on the ground or in an intermediate pose.
\paragraph{FDD dataset} is composed of 191 videos of which 107 are falls, recorded from a surveillance camera perspective by a single RGB camera. With every frame annotated between fall and not fall.

For the sake of comparison with other works we evaluated our method by three metrics: (i) Sensitivity reflecting the true positive rate, (ii) Specificity also known as the true negative rate, and (iii) Accuracy representing the overall performance of the method. We used a learning rate of $10^{-4}$, 500 epochs, 5-fold cross-validation, minibatches of $2^{10}$, and Adam optimization. We also experimented with different class weights, however, with further tuning of the hyper-parameters we were able to keep both classes on the same weight. The ensemble was performed by the double-SVM approach explained in Section~\ref{sec:classification}.

Tables~\ref{tab:urfd-ensem} and~\ref{tab:urfd-our-their} shows the results obtained over the URFD dataset. In Table~\ref{tab:urfd-ensem} we compare each single-stream feature with their multi-stream counterpart. The higher single-stream accuracy was derived from the optical flow stream, at 97.57\%, and the multi-stream was still able to outperform it, at 98.84\%. Table~\ref{tab:urfd-our-their} compares our result with other works in the literature, whilst not all works measured their sensitivity and specificity, all of them reported their accuracy. Our method's accuracy and specificity performed better than theirs, and kept the same 100\% sensitivity.

\begin{table*}[t]
\centering
\caption{URFD single-stream \textit{vs} multi-stream, decreasing accuracy.}
\label{tab:urfd-ensem}
\begin{tabular}{llcccl}
\hline
 &                              & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     & \\ \hline
 & Multi-stream (OF+RGB+Sal)    & \textbf{100}      & \textbf{98.77}    & \textbf{98.84}    & \\
 & Single-stream (OF)           & \textbf{100}      & 97.42             & 97.57             & \\
 & Single-stream (RGB)          & \textbf{100}      & 96.35             & 96.56             & \\
 & Single-stream (Sal)          & 93.67             & 93.39             & 93.40             & \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{URFD our method \textit{vs} literature, decreasing accuracy.}
\label{tab:urfd-our-their}
\begin{tabular}{llcccl}
\hline
 &                                                      & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     & \\ \hline
 & Lu et al.~\cite{lu2018deep}                          & -                 & -                 & \textbf{99.27}    & \\
 & Ours                                                 & \textbf{100}      & \textbf{98.77}    & 98.84             & \\
 & Panahi et al.~\cite{panahi2018human}                 & 97.05             & 97.02             & 97.14             & \\
 & Zerrouki and Houacine~\cite{zerrouki2018combined}    & -                 & -                 & 96.88             & \\
 & Harrou et al.~\cite{harrou2017vision}                & -                 & -                 & 96.66             & \\
 & Abobakr et al.~\cite{abobakr2017skeleton}            & \textbf{100}      & 0.91              & 96.00             & \\
 & Bhandari et al.~\cite{bhandari2017novel}             & 96.66             & -                 & 95.71             & \\
 & Kwolek and Kepski~\cite{kwolek2015improving}         & \textbf{100}      & 92.50             & 95.71             & \\
 & N\'u\~nez-Marcos et al.~\cite{nunez2017vision}       & \textbf{100}      & 92.00             & 95.00             & \\
 & Sase et al.~\cite{sase2018human}                     & 81.00             & -                 & 90.00             & \\ \hline
\end{tabular}
\end{table*}

Tables~\ref{tab:fdd-ensem} and~\ref{tab:fdd-our-their} reports the results extracted from the FDD dataset. Homologous to the URFD report, Table~\ref{tab:fdd-ensem} compares each single-stream with the multi-stream approach. The multi-stream achieved 99.51\% accuracy, and the highest single-stream score was again the optical flow, with 99.11\%. When comparing our results with the literature, in Table~\ref{tab:fdd-our-their}, our's outperformed every other one, regarding accuracy and sensitivity. And was 0.05\% worse then Charfi's et al.~\cite{charfi2013optimised} specificity. Our results also confirmed the findings of Wang et al.~\cite{wang2015towards}, since each feature stream did not perform great on their own, but when applyed together they contributed to state of the art results towards fall detection.

\begin{table*}[t]
\centering
\caption{FDD single-stream \textit{vs} multi-stream, decreasing accuracy.}
\label{tab:fdd-ensem}
\begin{tabular}{llcccl}
\hline
 &                      & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     &  \\ \hline
 & Multi (OF+RGB+Sal)   & \textbf{99.43}    & \textbf{99.55}    & \textbf{99.51}    &  \\
 & Single (OF)          & 99.07             & 99.12             & 99.11             &  \\
 & Single (RGB)         & 98.85             & 98.41             & 98.55             &  \\
 & Single (Sal)         & 88.39             & 92.40             & 91.25             & \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{FDD our method \textit{vs} literature, decreasing accuracy.}
\label{tab:fdd-our-their}
\begin{tabular}{llcccl}
\hline
 &                                                      & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     & \\ \hline
 & Ours                                                 & \textbf{99.43}    & 99.55             & \textbf{99.51}    & \\
 & Lu et al.~\cite{lu2018deep}                          & -                 & -                 & 99.36             & \\
 & Sehairi et al.~\cite{sehairi2018elderly}             & -                 & -                 & 98.91             & \\
 & Zerrouki and Houacine~\cite{zerrouki2018combined}    & -                 & -                 & 97.02             & \\
 & Harrou et al.~\cite{harrou2017vision}                & -                 & -                 & 97.02             & \\
 & Charfi et al.~\cite{charfi2012definition}            & 98.0              & \textbf{99.60}    & -                 & \\
 & N\'u\~nez-Marcos et al.~\cite{nunez2017vision}       & 99.0              & 97.00             & 97.00             & \\ \hline
\end{tabular}
\end{table*}
 
\section{Conclusion and Future Work}

This paper proposes to detect human fall events, based on the employment of a multi-stream convolutional neural network. Each stream was served an extracted feature, them being: (i) optical flow, (ii) saliency map, and (iii) the RGB frame itself, the ensemble was a double SVM approach. The proposed method was tested on URFD and FDD datasets, and our solution outperformed the related literature, indicating that multi-stream approaches and the selected features can be useful to detect human fall events.

It is worth discussing the selected features and some literature results. As et al.~\cite{chernbumroong2012elderly} noted that privacy concerns might arise on video-based approaches, the selected features are able to conceal one's identity. Despite these feature's performance, both optical flow and saliency map are timely and computational intensive to extract, which would halt their implementation in a real-time system. Considering this work's goal in exploring features to the fall detection problem, it is relevant to weight their extraction process against their results. And regarding other works in the literature, some seemed to dismiss the importance of the sensitivity metric, which quantifies the avoidance of false negatives. A World Health Organization report~\cite{who2007report}, enlightens the severity of an elderly fall, thus the importance of a high sensitivity metric.

Observing the results in Table~\ref{tab:urfd-our-their} and~\ref{tab:fdd-our-their}, it is possible to notice a saturation of these datasets, and as reported by et al.~\cite{chernbumroong2012elderly} systems trained on artificial environments did not perform as well on real-life scenarios. Therefore, the importance of future work to come up with a more extensive dataset that better resembles the actual application setup. Methods wise, as explained above, the exploration of cheaper features and deep networks that could generate comparable results, should also be focused on.

\section*{Acknowledgment}

%\begin{thebibliography}{00}
%\end{thebibliography}

\bibliographystyle{IEEEtran}
\bibliography{paper}

\end{document}