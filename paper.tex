\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Paper Title*\\
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}

\section{Related Work}

\section{Background}

\subsection{Optical Flow}

\subsection{Saliency Map}
\label{sec:saliency}

Why were you selected?

\section{Method}

An ensemble of VGG-16s were employed and evaluated to classify fall and not fall in video sequences, as seen in Figure~\ref{fig:overview} and further detailed bellow.

% In this work we intended to employ and evaluate a CNN architecture capable of detecting the presence or not of falls in videos, thus approaching the problem as a binary classification. To this end, we select the CNN architecture VGG16, since it is well known, and yields satisfactory results. Furthermore, based on the results of Wang et al.~\cite{wang2015towards}, in which a multi-streamed CNN outperformed single-stream ones, a ensembled was also exerted to learn the influence of three selected streams, each stream fed with a hand-crafted feature, them being: (i) optical flow, (ii) rgb, and (iii) saliency map.

\subsection{Transfer Learning}

The selected datasets, further detailed in Section~\ref{sec:experiments}, possesses a relatively small amount of human fall samples in their videos, which would limit the CNNs learning capabilities to detect a fall event. Thus, the need to transfer learning from other datasets.

The first 14 layers of the VGG-16 architecture were trained on the ImageNet~\cite{imagenet_cvpr09} dataset, and later on, these same layers were fine-tuned over the UCF101~\cite{soomro2012ucf101} dataset. The reasoning being that the network would initially learn low level features from the ImageNet classes, and focus these features on human activities with the UCF101 videos, Figure~\ref{fig:pre14}.

The 14 initial layer's weights were then frozen, and the last two fully connected layers were separately fine-tuned, with dropout regularization, over the extracted hand-crafted features of the URFD~\cite{kepski2014human} and FDD~\cite{charfi2013optimised} datasets. The last fine-tuning was performed three times, one for each of the extracted hand-crafted features, Figure~\ref{fig:pos14}.

\subsection{Classification}
\label{sec:classification}

Wang et al.~\cite{wang2015towards} reported that networks employing multi-stream classifiers outperformed their single-streamed peers. Inspired on this finding, a SVM classifier fed with the outputs of each stream, between 0 and 1, generates a ($x$, $y$, $z$) vector. This vector is then fed to a second SVM that classifies between fall and not fall, Figure~\ref{fig:pos14}.

\subsection{Streams Input}

Two selected features were fed to the streams explained in Subsection~\ref{sec:classification}:(i) Optical Flow and (ii) Saliency Map. Furthermore, the RGB frame itself was fed to the third stream, Figure~\ref{fig:features}, given the neural networks capability to learn relevant features to describe their input.

\paragraph{Optical Flow} represents the temporal relation between frames, and to fully gather this property it was used as a stack of optical flows on top of a sliding window of frames. The sliding window has a size of $L$ frames and gathers 2$L$ components ($L$ horizontal ($d_t^x$) + $L$ vertical ($d_t^y$) optical flow component vector fields) creating a stack $O$ = \{$d_t^x$, $d_t^y$, $d_{t+1}^x$, $d_{t+1}^y$, ..., $d_{t+L}^x$, $d_{t+L}^y$ \}. The total number of stacks is given by $N$-$L$+$1$, where $N$ is the number of frames in a video, Figure~\ref{fig:stacks, fig:features}. 
\paragraph{Saliency Map} is composed of pixels varying from 0 to 1, and was fed as a gray-scale image to the network, Figure~\ref{fig:features}.

\section{Experiments}
\label{sec:experiments}

Our experiments were performed on two publicly available datasets, \textit{URFD}~\cite{kepski2014human} and \textit{FDD}~\cite{charfi2013optimised}.

\paragraph{URFD dataset} contains 70 videos: (i) 30 videos of falls; and (ii) 40 videos of daily living activities, recorded from a top-down and side view perspective by two Microsoft Kinect cameras. Each frame is labeled in three categories, whether the person is lying on the ground, not lying on the ground or in an intermediate pose.
\paragraph{FDD dataset} is composed of 191 videos of which 107 are falls, recorded from a surveillance camera perspective by a single RGB camera. With every frame annotated between fall and not fall.

For the sake of comparison with other works in the literature we evaluated our method by three metrics: (i) Sensitivity reflecting the number of true positives, (ii) Specificity also known as the true negative rate, and (iii) Accuracy representing the overall performance of the method.

We experimented with different class weights, and with further tunning of the hyper parameters we were able to set all classes with the same weight.

learning rate 0.0001, kfold 5, minibatches of 1024, 500 epochs, optimization with adam, each fold result for the three stream combination

The ensemble was performed by the svm approach discussed in section

Compared ensembles techniques, SVM vs avarage

Each individual stream does not perform greatly when used alone, but the ensemble of them has a significant improved result

And the following results were obtained in the FDD dataset

The false negative cases must be taken care since they would be responsible for serious injuries and such

The following results were obatined in the URFD dataset

\begin{table*}[t]
\centering
\caption{URFD comparison between individuas streams and ensemble.}
\label{tab:urfd-ensem}
\begin{tabular}{llcccl}
\hline
 &  & Sensitivity (\%) & Specificity (\%) & Accuracy (\%) &  \\ \hline
 & Multi-stream (OF+RGB+Sal) & \textbf{100} & \textbf{98.77} & \textbf{98.84} &  \\
 & Single-stream (OF) & \textbf{100} & 97.42 & 97.57 &  \\
 & Single-stream (RGB) & \textbf{100} & 96.35 & 96.56 &  \\
 & Single-stream (Sal) & 93.67 & 93.39 & 93.40 & \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{URFD comparison between results.}
\label{tab:urfd-our-their}
\begin{tabular}{llcccl}
\hline
 &                              & Sensitivity (\%) & Specificity (\%) & Accuracy (\%)  &  \\ \hline
 & Ours                         & \textbf{100}      & \textbf{98.77}   & \textbf{98.84} &  \\
 & N\'u\~nez-Marcos et al.~\cite{nunez2017vision}      & \textbf{100}      & 92.00            & 95.00          &  \\
 & Zerrouki and Houacine~\cite{zerrouki2018vision}  & -                & -                & 96.88          &  \\
 & Bhandari et al.~\cite{bhandari2017novel}  & 96.66                & -                & 95.71          &  \\
 & Harrou et al.~\cite{harrou2017vision}  & -                & -                & 96.66          &  \\
 & Sase et al.~\cite{sase2018human}  & 81.00                & -                & 90.00          &  \\
 & Kwolek and Kepski~\cite{kepski2014human}            & \textbf{100}     & 92.50            & 95.71          &  \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{FDD comparison between individuas streams and ensemble.}
\label{tab:fdd-ensem}
\begin{tabular}{llcccl}
\hline
 &                      & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     &  \\ \hline
 & Multi (OF+RGB+Sal)   & \textbf{99.43}    & \textbf{99.55}    & \textbf{99.51}    &  \\
 & Single (OF)          & 99.07             & 99.12             & 99.11             &  \\
 & Single (RGB)         & 98.85             & 98.41             & 98.55             &  \\
 & Single (Sal)         & 88.39             & 92.40             & 91.25             & \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{FDD comparison between results.}
\label{tab:fdd-our-their}
\begin{tabular}{llcccl}
\hline
 &                          & Sensitivity (\%)  & Specificity (\%)  & Accuracy (\%)     & \\ \hline
 & Ours                     & \textbf{99.43}    & 99.55             & \textbf{99.51}    & \\
 & N\'u\~nez-Marcos et al.~\cite{nunez2017vision}  & 99.0              & 97.00             & 97.00             & \\
 & Zerrouki and Houacine~\cite{zerrouki2018vision}    & -                 & -                 & 97.02             & \\
 & Harrou et al.~\cite{harrou2017vision}    & -                 & -                 & 97.02             & \\
 & Charfi et al.~\cite{charfi2013optimised}            & 98.0              & \textbf{99.60}    & -                 & \\ \hline
\end{tabular}
\end{table*}

 It is worth nothing that in health segments, as a general rule it is usually preferred to accuse a false negative over a false positive, for instance it is considered that a HIV test with a false positive result will cause major distress on the patient's life, and if that result were a false negative, eventually the decease shymtons would demand another test. This general rule doesn't stand in our scenario, even tough a false positive would trigger a help dispatch which could cause some hassle to the patient, the aftereffects of a fall are related to the time between a fall and the help arrival, as observed by et al.~\cite{}.
 
\section{Future Work}



\section*{Acknowledgment}

%\begin{thebibliography}{00}
%\end{thebibliography}

\bibliographystyle{IEEEtran}
\bibliography{paper}

\color{red}
\centering
EIGHT PAGES

\end{document}